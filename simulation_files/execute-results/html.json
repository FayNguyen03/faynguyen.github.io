{
  "hash": "2146bf908bbda906f0c1aca106a7d3d3",
  "result": {
    "markdown": "---\ntitle: \"Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(patchwork)\nlibrary(forcats)\n```\n:::\n\n\n## Overview:\n\n**Power of a statistical test**: the probability that it rejects the null hypothesis when the null hypothesis is false or the probability that a statistical test can detect when a true difference exists\n\nThere are some factors that affect the power of a statistical test:\n\n1.  Variability in the data (variance)\n\n2.  True difference\n\n3.  Type I Error Level\n\n4.  Size of the sample\n\nWhile we evaluate the effect of these factors on the power, we will especially consider the intercept at 0.8. Power is usually set at 80%. This means that if there are true effects to be found in 100 different studies with 80% power, only 80 out of 100 statistical tests will actually detect them. The reason why we choose this value is that the minimum power of a study required is 80%.\n\nBelow is a generalized function to compute the power of each statistical test based on the four important factors (size of sample, type I error level or significance leve, standard deviation, and difference in mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npower_stat_test <- function(mean1, mean_diff, sd, type_1_error_level, numsims,iter){\n  #list of power values after all iterations \n  power_vals <- vector(\"double\",iter)\n  #list of sample size value\n  sample_size <- vector(\"numeric\",iter)\n  #calculate the mean of the second group\n  mean2 <- mean1 + mean_diff\n  #calculate the standard deviation of the second group\n  for (k in 1:iter)  {\n    significant <- vector(\"logical\", numsims)\n    n1 <- 5 * k\n    n2 <- n1\n    for (i in 1:numsims) {\n      samp1 <- rnorm(n1, mean1, sd)\n      samp2 <- rnorm(n2, mean2, sd)\n      p_value <- t.test(x = samp1, y = samp2)$p.value\n      significant[i] <- (p_value < type_1_error_level)\n    }\n    power_vals[k] <- mean(significant)\n    sample_size[k] <- n1\n  }\n  return(list(sample_size,power_vals))\n}\n```\n:::\n\n\nThis function takes these parameters:\n\n| Parameter            | Purpose                                                                                                                                  |\n|------------------------|-----------------------------------------------|\n| `mean1`              | The mean value of the first group                                                                                                        |\n| `mean_diff`          | The true difference in mean value of two groups                                                                                          |\n| `sd`                | The standard deviation of two groups                                                                                              |\n| `type_1_error_level` | Type I error level or the risk of making a Type I error                                                                                  |\n| `numsims`            | Number of statistics tests on the same simulated data set (=1000)                                                                        |\n| `iter`               | This value used for the purpose of testing the simulated data set with a factor changing over the time and other factors remain the same |\n\nSome parameters can be constant between tests so I will initialize it here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean1 <- 225\niter <- 10\ntype_1_error_level <- 0.05\nnumsims <- 1000\nsd <- 35\n```\n:::\n\n\n## Factor 1: Variance\n\nThe first factor we are going to examine is **variance**. We will evaluate $\\sigma=\\sqrt{Variance}$.\n\nThis is a list of randomized standard deviation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd_diff <- sample(5:75,iter)\nprint(sd_diff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 61 65 60 21 47 69 37 22 54 25\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#A list storing the results\ncombined_data <- tibble(n_per_group = numeric(), power = numeric(), sd_diff_val = numeric())\nfor (val in sd_diff) {\n  result <- power_stat_test(mean1, 0, val, type_1_error_level, numsims, iter)\n  temp_data <- tibble(n_per_group = result[[1]], power = result[[2]], sd_diff_val = rep(val, length(result[[1]])))\n  combined_data <- rbind(combined_data, temp_data)\n}\n\nggplot(combined_data, aes(x = n_per_group, y = power, color = as.factor(sd_diff_val))) +\n  geom_line() +\n  geom_hline(yintercept = 0.8, color = \"red\") +\n  labs(color = \"SD Difference\")+\n  theme_minimal() +\n  labs(x=\"Sample Size\", y = \"Power\", color=\"Standard deviation\",title=\"Power of statistical tests with different standard deviation\")\n```\n\n::: {.cell-output-display}\n![](simulation_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThough the power of each case is approximately the same, according to the legend that is sorted based on the power and the sample size, the cases with smaller deviation seems to have larger power. \n\nExperimenters can  control the standard deviation by sampling a homogeneous population of subjects, by reducing random measurement error and/or by making sure the experimental procedures are applied very consistent.\n\n## Factor 2: Size of the true difference\n\nNext, we will evaluate the factor true difference, which is $|\\Delta_{mean1-mean2}|$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrue_diff <- sample(0:100,iter)\nprint(true_diff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 49 62 70 57 99 13 54 12 39 85\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncombined_data_mean <- tibble(n_per_group = numeric(), power = numeric(), true_diff_val = numeric())\nfor (val in true_diff) {\n  result <- power_stat_test(mean1, val, sd, type_1_error_level, numsims, iter)\n  temp_data <- tibble(n_per_group = result[[1]], power = result[[2]], true_diff_val = rep(val, length(result[[1]])))\n  combined_data_mean <- rbind(combined_data_mean, temp_data)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(combined_data_mean, \n       aes(x = n_per_group, \n           y = power,\n       color=fct_reorder(as.factor(true_diff_val),power))) +\n  geom_line(linewidth=0.8)+\n  geom_hline(yintercept = 0.8, color = \"red\", linewidth=1) +\n  labs(x=\"Sample Size\", \n       y = \"Power\", \n       color=\"True Difference\",\n       title=\"Power of statiscal tests with different true difference\")+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](simulation_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nFrom the graph, we can see that the higher the true difference is, the higher the power is. As the sample size grows larger with significant true difference, the power approaches 1 faster.\n\n## Factor 3: Type 1 Error Level\n\nThe third factor we will evaluate is Type I Error Level $\\alpha$.\n\nThis is a list of randomized Type I Error Level.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntype_1_error_level <- sample(seq(0.01, 0.5, by = 0.01), iter, replace = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncombined_data_error <- tibble(n_per_group = numeric(), power = numeric(), error_level = numeric())\ntype_1_error_level\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 0.30 0.36 0.24 0.11 0.22 0.50 0.20 0.39 0.38 0.44\n```\n:::\n\n```{.r .cell-code}\nfor (val in type_1_error_level) {\n  result <- power_stat_test(mean1,10, sd, val, numsims, iter)\n  temp_data <- tibble(n_per_group = result[[1]], power = result[[2]], error_level = rep(val, length(result[[1]])))\n  combined_data_error <- rbind(combined_data_error, temp_data)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(combined_data_error, aes(x = n_per_group, y = power, color = fct_reorder2(as.factor(error_level),n_per_group,power))) +\n  geom_line(linewidth=0.8) +\n  geom_hline(yintercept = 0.8, color = \"red\", linewidth=1) +\n  labs(color = \"Type I Error\")+\n  labs(x=\"Sample Size\", \n       y = \"Power\", \n       color=\"Type I Error\",\n       title=\"Power of statiscal tests with different true difference\")+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](simulation_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nThe relationship between the Type I Error Level and power: the more the the Type I Error Level is, the higher the power is. The stronger the evidence needed to reject the null hypothesis, the lower the chance that the hypothesis will be rejected.\n\n## Factor 4: Sample size\n\nFrom those graphs above, we can see an obvious pattern that the larger the sample size, the higher the power with other factors remain the same. Since sample size is typically under an experimenter's control, increasing sample is one way to increase power.\n\n\n",
    "supporting": [
      "simulation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}