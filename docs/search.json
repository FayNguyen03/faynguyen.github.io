[
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Fay’s Data Science Portfolio",
    "section": "",
    "text": "Landing My First Internship in Industry\n\n\n\n\nBA in Computer Science, Math\nStatistics and Data Science concentration\n2026 | St. Olaf College\n\n\n\n\nUndergraduate Researcher | May 2023 - Present\nLead CS Teaching Assistant | Feb 2023 - Present\nTechnology Consulting Assistant | Sep 2022 - Present\n\n\n\n\n\nCourse Link\nPortfolio\n\n\n\n\nEmail: nguyen94@stolaf.edu"
  },
  {
    "objectID": "index.html#goals",
    "href": "index.html#goals",
    "title": "Fay’s Data Science Portfolio",
    "section": "",
    "text": "Landing My First Internship in Industry"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Fay’s Data Science Portfolio",
    "section": "",
    "text": "BA in Computer Science, Math\nStatistics and Data Science concentration\n2026 | St. Olaf College"
  },
  {
    "objectID": "map.html",
    "href": "map.html",
    "title": "Mini Project 1: Maps",
    "section": "",
    "text": "us_data &lt;- map_data(\"state\")\nhome_health_care &lt;- read.csv(\"dataset/health.csv\")\n\nhome_health_care |&gt;\n  drop_na(Quality.of.Patient.Care.Star.Rating) |&gt;\n  mutate(State = str_to_lower(abbr2state(State))) |&gt;\n  filter(!is.na(State), !State %in% c(\"district of columbia\",\"alaska\",\"hawaii\")) |&gt;\n  right_join(us_data,\n    by = c(\"State\" = \"region\")) |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = How.often.patients..wounds.improved.or.healed.after.an.operation),linewidth=0.3,color=\"white\") +\n  labs(fill = \"Percentage of wounds getting healed \\nafter operation\", title=\"How Often Patients' Wounds Healed After An Operation In 48 States\", subtitle=\"Data: Home Health Care - State by State Data\",caption = \"Source: Data.gov\") +\n  coord_map() +\n  theme(plot.title = element_text(size = 30)) +\n  theme_void() +\n  scale_fill_viridis()\n\n\n\n\n\n\n\n\nData Link\nThe data set records the state averages of several home health agency quality measures for Home Health Agencies. The variable of interest is\nHow.often.patients..wounds.improved.or.healed.after.an.operation.\nIn the plot, the more yellowish shade each state has, the higher the average percentage of wounds healed after operation. Therefore, South Dakota is the state with the lowest percentage of healed wounds (&lt;87%) while Rhode Island is the highest (&gt;93%). The states in the Northeastern and South East regions tend to have higher percentage of healing than the other regions of the country. This plot can provide valuable insights into the healthcare quality of each state. However, we need to carefully consider various factors beyond the raw averages to accurately assess and compare healthcare quality between states since each state has different population demographics as well as patients’ access to healthcare services and how frequently individuals utilize these services can impact healing rates. Therefore, while state averages for wound healing after operations provide an important snapshot of healthcare outcomes of each state, they should be interpreted within the broader context of demographic, socioeconomic, policy, and healthcare system factors.",
    "crumbs": [
      "Mini Project 1: Maps"
    ]
  },
  {
    "objectID": "map.html#us-states",
    "href": "map.html#us-states",
    "title": "Mini Project 1: Maps",
    "section": "",
    "text": "us_data &lt;- map_data(\"state\")\nhome_health_care &lt;- read.csv(\"dataset/health.csv\")\n\nhome_health_care |&gt;\n  drop_na(Quality.of.Patient.Care.Star.Rating) |&gt;\n  mutate(State = str_to_lower(abbr2state(State))) |&gt;\n  filter(!is.na(State), !State %in% c(\"district of columbia\",\"alaska\",\"hawaii\")) |&gt;\n  right_join(us_data,\n    by = c(\"State\" = \"region\")) |&gt;\n  ggplot(mapping = aes(x = long, y = lat, group = group)) +\n  geom_polygon(aes(fill = How.often.patients..wounds.improved.or.healed.after.an.operation),linewidth=0.3,color=\"white\") +\n  labs(fill = \"Percentage of wounds getting healed \\nafter operation\", title=\"How Often Patients' Wounds Healed After An Operation In 48 States\", subtitle=\"Data: Home Health Care - State by State Data\",caption = \"Source: Data.gov\") +\n  coord_map() +\n  theme(plot.title = element_text(size = 30)) +\n  theme_void() +\n  scale_fill_viridis()\n\n\n\n\n\n\n\n\nData Link\nThe data set records the state averages of several home health agency quality measures for Home Health Agencies. The variable of interest is\nHow.often.patients..wounds.improved.or.healed.after.an.operation.\nIn the plot, the more yellowish shade each state has, the higher the average percentage of wounds healed after operation. Therefore, South Dakota is the state with the lowest percentage of healed wounds (&lt;87%) while Rhode Island is the highest (&gt;93%). The states in the Northeastern and South East regions tend to have higher percentage of healing than the other regions of the country. This plot can provide valuable insights into the healthcare quality of each state. However, we need to carefully consider various factors beyond the raw averages to accurately assess and compare healthcare quality between states since each state has different population demographics as well as patients’ access to healthcare services and how frequently individuals utilize these services can impact healing rates. Therefore, while state averages for wound healing after operations provide an important snapshot of healthcare outcomes of each state, they should be interpreted within the broader context of demographic, socioeconomic, policy, and healthcare system factors.",
    "crumbs": [
      "Mini Project 1: Maps"
    ]
  },
  {
    "objectID": "map.html#wisconsin-districts",
    "href": "map.html#wisconsin-districts",
    "title": "Mini Project 1: Maps",
    "section": "Wisconsin Districts",
    "text": "Wisconsin Districts\n\n\nWarning: package 'sf' was built under R version 4.3.3\n\n\n\nwi &lt;- ggplot(data = wi_merged, aes(fill = winner)) +\n  annotation_map_tile(zoom = 6, type = \"osm\", progress = \"none\") + \n  geom_sf(alpha = 0.5) +\n  scale_fill_manual(\"Winner\", values = c(\"blue\", \"red\")) + \n  geom_sf_label(aes(label = DISTRICT), fill = \"white\") +\n  theme_void()\nwi + \n  labs(fill = \"Winner Party\", title=\"Wisconsin Controversial Congressional Districts and Gerrymandering\\nin 2016\", caption = \"Source: fes16\")\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\nwi +\n  aes(fill = r_prop) + \n  scale_fill_distiller(\n    \"Proportion\\nRepublican\", \n    palette = \"RdBu\", \n    limits = c(0, 0.7)\n  )+ \n  labs(title=\"Proportion of Republican Votes in Wisconsin Congressional Districts\", subtitle =\"In 2016\", caption = \"Source: fes16\") \n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\n\n\n\n\n\n\n\n\nFor the choropleth map, the district 4 has color blue.\n\npal &lt;- colorNumeric(palette = \"RdYlBu\", domain = c(0, 1))\n\nleaflet_wi &lt;- leaflet(wi_merged) |&gt;\n  addTiles() |&gt;\n  addPolygons(\n    weight = 1, \n    fillOpacity = 0.7, \n    fillColor = ~ifelse(r_prop &gt; d_prop, \"red\", \"blue\"), \n    color = \"white\",\n    popup = ~paste(\"District \", DISTRICT, \"&lt;/br&gt;\", \"Probability of Republic Votes: \", round(r_prop, 4),\"&lt;/br&gt;\", \"Probability of Democratic Votes: \", round(d_prop, 4), \"&lt;/br&gt;Total votes: \", wi_results$total_votes)\n  ) |&gt;                         \n  setView(lng = -90, lat = 45, zoom = 7)\nleaflet_wi\n\n\n\n\n\nClicking on a district will show the probability of Republican and Democratic votes of that district.\nIn 2016, most of the districts in Wisconsin’s winner was Republic (6/8), except for District 2,3, and 4. In District 3 and District 4, there are no votes for Republican party. The ratio of probability of votes between the winner and the opponent party of most district is about 2:1. According to the plot, I assume that the winner party is Republican Party since it won more districts.",
    "crumbs": [
      "Mini Project 1: Maps"
    ]
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Link"
  },
  {
    "objectID": "projects.html#mini-project-1-maps",
    "href": "projects.html#mini-project-1-maps",
    "title": "Projects",
    "section": "",
    "text": "Link"
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Fay’s Data Science Portfolio",
    "section": "",
    "text": "Email: nguyen94@stolaf.edu"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Fay’s Data Science Portfolio",
    "section": "",
    "text": "Undergraduate Researcher | May 2023 - Present\nLead CS Teaching Assistant | Feb 2023 - Present\nTechnology Consulting Assistant | Sep 2022 - Present"
  },
  {
    "objectID": "index.html#link",
    "href": "index.html#link",
    "title": "Fay’s Data Science Portfolio",
    "section": "",
    "text": "Course Link\nPortfolio"
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(patchwork)\nlibrary(forcats)",
    "crumbs": [
      "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test"
    ]
  },
  {
    "objectID": "simulation.html#factor-1-variance",
    "href": "simulation.html#factor-1-variance",
    "title": "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test",
    "section": "Factor 1: Variance",
    "text": "Factor 1: Variance\nThe first factor we are going to examine is variance. We will evaluate \\(\\sigma=\\sqrt{Variance}\\).\nThis is a list of randomized standard deviation.\n\nsd_diff &lt;- sample(5:75,iter)\nprint(sd_diff)\n\n [1] 50 44 32 27 25 20 61 40 57  6\n\n\n\n#A list storing the results\ncombined_data &lt;- tibble(n_per_group = numeric(), power = numeric(), sd_diff_val = numeric())\nfor (val in sd_diff) {\n  result &lt;- power_stat_test(mean1, 0, val, type_1_error_level, numsims, iter)\n  temp_data &lt;- tibble(n_per_group = result[[1]], power = result[[2]], sd_diff_val = rep(val, length(result[[1]])))\n  combined_data &lt;- rbind(combined_data, temp_data)\n}\n\nggplot(combined_data, aes(x = n_per_group, y = power, color = as.factor(sd_diff_val))) +\n  geom_line() +\n  geom_hline(yintercept = 0.8, color = \"red\") +\n  labs(color = \"SD Difference\")+\n  theme_minimal() +\n  labs(x=\"Sample Size\", y = \"Power\", color=\"Standard deviation\",title=\"Power of statistical tests with different standard deviation\")\n\n\n\n\n\n\n\n\nThough the power of each case is approximately the same, according to the legend that is sorted based on the power and the sample size, the cases with smaller deviation seems to have larger power.\nExperimenters can control the standard deviation by sampling a homogeneous population of subjects, by reducing random measurement error and/or by making sure the experimental procedures are applied very consistent.",
    "crumbs": [
      "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test"
    ]
  },
  {
    "objectID": "simulation.html#factor-2-size-of-the-true-difference",
    "href": "simulation.html#factor-2-size-of-the-true-difference",
    "title": "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test",
    "section": "Factor 2: Size of the true difference",
    "text": "Factor 2: Size of the true difference\nNext, we will evaluate the factor true difference, which is \\(|\\Delta_{mean1-mean2}|\\).\n\ntrue_diff &lt;- sample(0:100,iter)\nprint(true_diff)\n\n [1] 67 21 47 49 33 16 54 22 23 75\n\n\n\ncombined_data_mean &lt;- tibble(n_per_group = numeric(), power = numeric(), true_diff_val = numeric())\nfor (val in true_diff) {\n  result &lt;- power_stat_test(mean1, val, sd, type_1_error_level, numsims, iter)\n  temp_data &lt;- tibble(n_per_group = result[[1]], power = result[[2]], true_diff_val = rep(val, length(result[[1]])))\n  combined_data_mean &lt;- rbind(combined_data_mean, temp_data)\n}\n\n\nggplot(combined_data_mean, \n       aes(x = n_per_group, \n           y = power,\n       color=fct_reorder(as.factor(true_diff_val),power))) +\n  geom_line(linewidth=0.8)+\n  geom_hline(yintercept = 0.8, color = \"red\", linewidth=1) +\n  labs(x=\"Sample Size\", \n       y = \"Power\", \n       color=\"True Difference\",\n       title=\"Power of statiscal tests with different true difference\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nFrom the graph, we can see that the higher the true difference is, the higher the power is. As the sample size grows larger with significant true difference, the power approaches 1 faster.",
    "crumbs": [
      "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test"
    ]
  },
  {
    "objectID": "simulation.html#factor-3-type-1-error-level",
    "href": "simulation.html#factor-3-type-1-error-level",
    "title": "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test",
    "section": "Factor 3: Type 1 Error Level",
    "text": "Factor 3: Type 1 Error Level\nThe third factor we will evaluate is Type I Error Level \\(\\alpha\\).\nThis is a list of randomized Type I Error Level.\n\ntype_1_error_level &lt;- sample(seq(0.01, 0.5, by = 0.01), iter, replace = TRUE)\n\n\ncombined_data_error &lt;- tibble(n_per_group = numeric(), power = numeric(), error_level = numeric())\ntype_1_error_level\n\n [1] 0.50 0.27 0.32 0.28 0.17 0.20 0.04 0.24 0.21 0.33\n\nfor (val in type_1_error_level) {\n  result &lt;- power_stat_test(mean1,10, sd, val, numsims, iter)\n  temp_data &lt;- tibble(n_per_group = result[[1]], power = result[[2]], error_level = rep(val, length(result[[1]])))\n  combined_data_error &lt;- rbind(combined_data_error, temp_data)\n}\n\n\nggplot(combined_data_error, aes(x = n_per_group, y = power, color = fct_reorder2(as.factor(error_level),n_per_group,power))) +\n  geom_line(linewidth=0.8) +\n  geom_hline(yintercept = 0.8, color = \"red\", linewidth=1) +\n  labs(color = \"Type I Error\")+\n  labs(x=\"Sample Size\", \n       y = \"Power\", \n       color=\"Type I Error\",\n       title=\"Power of statiscal tests with different true difference\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe relationship between the Type I Error Level and power: the more the the Type I Error Level is, the higher the power is. The stronger the evidence needed to reject the null hypothesis, the lower the chance that the hypothesis will be rejected.",
    "crumbs": [
      "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test"
    ]
  },
  {
    "objectID": "simulation.html#factor-4-sample-size",
    "href": "simulation.html#factor-4-sample-size",
    "title": "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test",
    "section": "Factor 4: Sample size",
    "text": "Factor 4: Sample size\nFrom those graphs above, we can see an obvious pattern that the larger the sample size, the higher the power with other factors remain the same. Since sample size is typically under an experimenter’s control, increasing sample is one way to increase power.",
    "crumbs": [
      "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test"
    ]
  },
  {
    "objectID": "simulation.html#the-effects-of-two-factors-on-the-power-of-a-statistical-test",
    "href": "simulation.html#the-effects-of-two-factors-on-the-power-of-a-statistical-test",
    "title": "Mini Project 2: Simulation",
    "section": "The Effects of Two Factors On The Power of A Statistical Test",
    "text": "The Effects of Two Factors On The Power of A Statistical Test\nThere are some factors that affect the power of a statistical test:\n\nsample size\ntype I error level\nvariability in the data\nsize of the true difference\n\nwhy intercept at 0.8: To evaluate the effects for two factors, I created a generalized function to compute the power of each statistical test:\n\npower_stat_test &lt;- function(mean1, mean_diff, sd1, sd_diff, type_1_error_level, numsims,iter){\n  #list of power values after all iterations \n  power_vals &lt;- vector(\"double\",iter)\n  #list of sample size value\n  sample_size &lt;- vector(\"numeric\",iter)\n  #calculate the mean of the second group\n  mean2 &lt;- mean1 + mean_diff\n  #calculate the standard deviation of the second group\n  sd2 &lt;- sd1 + sd_diff\n  for (k in 1:iter)  {\n    significant &lt;- vector(\"logical\", numsims)\n    n1 &lt;- 5 * k\n    n2 &lt;- n1\n    for (i in 1:numsims) {\n      samp1 &lt;- rnorm(n1, mean1, sd1)\n      samp2 &lt;- rnorm(n2, mean2, sd2)\n      p_value &lt;- t.test(x = samp1, y = samp2)$p.value\n      significant[i] &lt;- (p_value &lt; type_1_error_level)\n    }\n    power_vals[k] &lt;- mean(significant)\n    sample_size[k] &lt;- n1\n  }\n  return(list(sample_size,power_vals))\n}\n\nThis function takes the parameters:\n\n\n\n\n\n\n\nParameter\nPurpose\n\n\n\n\nmean1\nThe mean value of the first group\n\n\nmean_diff\nThe true difference in mean value of two groups\n\n\nsd1\nThe standard deviation of the first group\n\n\nsd_diff\nThe difference in standard deviation of two groups\n\n\ntype_1_error_level\nType I error level or the risk of making a Type I error\n\n\nnumsims\nNumber of statistics tests on the same simulated data set (=1000)\n\n\niter\nThis value used for the purpose of testing the simulated data set with a factor changing over the time and other factors remain the same\n\n\n\nI set this value large to…\n\nmean1 &lt;- 225\niter &lt;- 10\ntype_1_error_level &lt;- 0.05\nnumsims &lt;- 1000\nsd1 &lt;- 35"
  },
  {
    "objectID": "simulation.html#overview",
    "href": "simulation.html#overview",
    "title": "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test",
    "section": "Overview:",
    "text": "Overview:\nPower of a statistical test: the probability that it rejects the null hypothesis when the null hypothesis is false or the probability that a statistical test can detect when a true difference exists\nThere are some factors that affect the power of a statistical test:\n\nVariability in the data (variance)\nTrue difference\nType I Error Level\nSize of the sample\n\nWhile we evaluate the effect of these factors on the power, we will especially consider the intercept at 0.8. Power is usually set at 80%. This means that if there are true effects to be found in 100 different studies with 80% power, only 80 out of 100 statistical tests will actually detect them. The reason why we choose this value is that the minimum power of a study required is 80%.\nBelow is a generalized function to compute the power of each statistical test based on the four important factors (size of sample, type I error level or significance leve, standard deviation, and difference in mean:\n\npower_stat_test &lt;- function(mean1, mean_diff, sd, type_1_error_level, numsims,iter){\n  #list of power values after all iterations \n  power_vals &lt;- vector(\"double\",iter)\n  #list of sample size value\n  sample_size &lt;- vector(\"numeric\",iter)\n  #calculate the mean of the second group\n  mean2 &lt;- mean1 + mean_diff\n  #calculate the standard deviation of the second group\n  for (k in 1:iter)  {\n    significant &lt;- vector(\"logical\", numsims)\n    n1 &lt;- 5 * k\n    n2 &lt;- n1\n    for (i in 1:numsims) {\n      samp1 &lt;- rnorm(n1, mean1, sd)\n      samp2 &lt;- rnorm(n2, mean2, sd)\n      p_value &lt;- t.test(x = samp1, y = samp2)$p.value\n      significant[i] &lt;- (p_value &lt; type_1_error_level)\n    }\n    power_vals[k] &lt;- mean(significant)\n    sample_size[k] &lt;- n1\n  }\n  return(list(sample_size,power_vals))\n}\n\nThis function takes these parameters:\n\n\n\n\n\n\n\nParameter\nPurpose\n\n\n\n\nmean1\nThe mean value of the first group\n\n\nmean_diff\nThe true difference in mean value of two groups\n\n\nsd\nThe standard deviation of two groups\n\n\ntype_1_error_level\nType I error level or the risk of making a Type I error\n\n\nnumsims\nNumber of statistics tests on the same simulated data set (=1000)\n\n\niter\nThis value used for the purpose of testing the simulated data set with a factor changing over the time and other factors remain the same\n\n\n\nSome parameters can be constant between tests so I will initialize it here:\n\nmean1 &lt;- 225\niter &lt;- 10\ntype_1_error_level &lt;- 0.05\nnumsims &lt;- 1000\nsd &lt;- 35",
    "crumbs": [
      "Mini Project 2: Simulation - The Effects of Two Factors On The Power of A Statistical Test"
    ]
  },
  {
    "objectID": "string_regex.html",
    "href": "string_regex.html",
    "title": "Mini Project 3: Strings and Regular Expressions",
    "section": "",
    "text": "BTS (Bangtan Sonyeondan or “Beyond the Scene”) is a Grammy-nominated South Korean group that has been capturing the hearts of millions of fans globally since their debut in June 2013.\n\nThe members of BTS are RM, Jin, SUGA, j-hope, Jimin, V, and Jung Kook. This 7-member K-Pop boy group gains recognition for their authentic and self-produced music, top-notch performances, and the way they interact with their fans as well as has established themselves as “21st century Pop Icons” breaking countless world records.\nThey always thrive to impart the positive impact to the youth in all around the world with meaningful activities such as the LOVE MYSELF campaign and the UN ‘Speak Yourself’ speech, which help them mobilize millions of fans - the ARMY - across the world.\nDuring 11 years, they have released more than 200 songs with a wide range of genres, with most of them composed and produced by the group members. The topics of their lyrics focus on subjects including mental health, the troubles of school-age youth and coming of age, loss, the journey towards self-love, individualism, and the consequences of fame and recognition.\nAs an ARMY, I want to utilize the text analysis skill I acquired from MSCS264 to conduct some data mining with their song lyrics and look forward to discover some meaningful pattern in the song lyrics.",
    "crumbs": [
      "Mini Project 3: Strings and Regular Expressions"
    ]
  },
  {
    "objectID": "string_regex.html#emotion",
    "href": "string_regex.html#emotion",
    "title": "Mini Project 3: Strings and Regular Expressions",
    "section": "Emotion",
    "text": "Emotion\n\nbts_lyrics_sentences &lt;- read.csv(\"dataset/lyrics-v12.csv\") %&gt;%\n  filter(lyrics != \"\", repackaged != TRUE, !str_detect(eng_track_title, \"Skit.*\")) %&gt;%\n  select(-kor_track_title, -id, -track_title, -has_full_ver, -repackaged) %&gt;%\n  mutate(\n    lyrics = str_replace_all(lyrics, \"[\\\\(\\\\{][^\\\\)\\\\}]*[\\\\)\\\\}]\", \"\"), \n    lyrics = str_replace_all(lyrics, \"[,\\\\*\\\\.!\\`“”‘’]\", \"\"),\n    lyrics = str_replace_all(lyrics, \"\\\"\", \"\") \n  ) \nbts_lyrics_sentences$lyrics[1]\n\n[1] \"were now going to progress to some steps\\nwhich are a bit more difficult\\nready set and begin\\nbig hit exclusive exclusive\\ndj friz\\nwhos that?\\nb a n g t a n\\nbulletproof boy scouts\\n\\n2 cool 2 cool 4 skool\\n2 cool 2 cool 4 skool\\nteenagers in their 10s and 20s\\nlet's talk about it easily\\nyou take that\\n2 cool 2 cool 4 skool\\n2 cool 2 cool 4 skool\\nteenagers in their 10s and 20s\\nlet's talk about it easily\\nyou take that\"\n\n\n\nstr_view(bts_lyrics_sentences$lyrics[2], '\\n')\n\n[1] │  give it to me&lt;\n    │ &gt;  be nervous&lt;\n    │ &gt;  the one to end it all&lt;\n    │ &gt;  we are bulletproof&lt;\n    │ &gt; we are bulletproof&lt;\n    │ &gt; bulletproof&lt;\n    │ &gt; &lt;\n    │ &gt; the name is jungkook my scale is nationwide&lt;\n    │ &gt; i pulled all-nighters at practice rooms&lt;\n    │ &gt; instead of school dancing and singing&lt;\n    │ &gt; while you guys partied&lt;\n    │ &gt; i gave up sleep for my dreams&lt;\n    │ &gt; i spent all night holding a pen&lt;\n    │ &gt; closing my eyes only after the morning sun rises&lt;\n    │ &gt; my limit was broken in the&lt;\n    │ &gt; double standards and many oppositions&lt;\n    │ &gt; but i got lucky and was contacted by an agency&lt;\n    │ &gt; yall you who are called rappers because you cant sing&lt;\n    │ &gt; the rapper title is an extravagance for you&lt;\n    │ &gt; &lt;\n    │ &gt; everywhere i go everything i do&lt;\n    │ &gt; i will show you as much as i sharpened my sword&lt;\n    │ &gt; to all the people who looked down on me&lt;\n    │ &gt;  hey shout it out&lt;\n    │ &gt; &lt;\n    │ &gt; oh throw a stone at me if youve done as much as i did&lt;\n    │ &gt; we go hard we have no fear&lt;\n    │ &gt;  we just sing it like&lt;\n    │ &gt;  we just sing it like&lt;\n    │ &gt; oh throw a stone at me if youve done as much as i did&lt;\n    │ &gt; we go hard we have no fear&lt;\n    │ &gt;  we just sing it like&lt;\n    │ &gt;  we just sing it like&lt;\n    │ &gt; &lt;\n    │ &gt;  give it to me&lt;\n    │ &gt;  be nervous&lt;\n    │ &gt;  the one to end it all&lt;\n    │ &gt;  we are bulletproof&lt;\n    │ &gt; we are bulletproof&lt;\n    │ &gt; bulletproof&lt;\n    │ &gt; &lt;\n    │ &gt; look at my profile theres nothing there yet&lt;\n    │ &gt; still a trainee and rapper man&lt;\n    │ &gt; yeah i do know thats nothing&lt;\n    │ &gt; i contemplated a lot but now i dont need anything&lt;\n    │ &gt; youre still an amateur&lt;\n    │ &gt; im in the majors hope youll rot like that&lt;\n    │ &gt; rap monster like my name&lt;\n    │ &gt; i can eat up any kind of beat like a monster&lt;\n    │ &gt; im loyal to my name guys come here take a preview&lt;\n    │ &gt; i put a twist to being an idol&lt;\n    │ &gt; haha hyungs who only had hip-hop pride&lt;\n    │ &gt; told me itd be impossible but&lt;\n    │ &gt; look carefully i place a period after impossible&lt;\n    │ &gt; im possible now are we all set boy?&lt;\n    │ &gt; &lt;\n    │ &gt; everywhere i go everything i do&lt;\n    │ &gt; i will show you as much as i sharpened my sword&lt;\n    │ &gt; to all the people who looked down on me&lt;\n    │ &gt;  hey shout it out&lt;\n    │ &gt; &lt;\n    │ &gt; oh throw a stone at me if youve done as much as i did&lt;\n    │ &gt; we go hard we have no fear&lt;\n    │ &gt;  we just sing it like&lt;\n    │ &gt;  we just sing it like&lt;\n    │ &gt; oh throw a stone at me if youve done as much as i did&lt;\n    │ &gt; we go hard we have no fear&lt;\n    │ &gt;  we just sing it like&lt;\n    │ &gt;  we just sing it like&lt;\n    │ &gt; &lt;\n    │ &gt;  give it to me&lt;\n    │ &gt;  be nervous&lt;\n    │ &gt;  the one to end it all&lt;\n    │ &gt;  we are bulletproof&lt;\n    │ &gt; we are bulletproof&lt;\n    │ &gt; bulletproof&lt;\n    │ &gt; bulletproof\n\nsentenceLyricsList &lt;- str_split(bts_lyrics_sentences$lyrics,\"\\n\")\nsentenceLyricsList &lt;- unlist(sentenceLyricsList) \nwordsLyricsList &lt;- wordsLyricsList[str_detect(wordsLyricsList,\"[A-Za-z']+$\")]\nwordFreq &lt;- as.data.frame(table(wordsLyricsList)) |&gt;\n  mutate(wordsLyricsList = as.character(wordsLyricsList)) |&gt;\n  anti_join(stopwordsList,by=c(wordsLyricsList=\"value\")) |&gt;\n  slice_max(Freq,n=300)"
  },
  {
    "objectID": "string_regex.html#word-distribution-of-the-three-favorite-albums",
    "href": "string_regex.html#word-distribution-of-the-three-favorite-albums",
    "title": "Mini Project 3: Strings and Regular Expressions",
    "section": "Word Distribution of THe Three Favorite Albums",
    "text": "Word Distribution of THe Three Favorite Albums"
  },
  {
    "objectID": "string_regex.html#what-are-the-words-used-that-used-most-in-bts-lyrics",
    "href": "string_regex.html#what-are-the-words-used-that-used-most-in-bts-lyrics",
    "title": "Mini Project 3: Strings and Regular Expressions",
    "section": "What are the words used that used most in BTS’ lyrics?",
    "text": "What are the words used that used most in BTS’ lyrics?\nThe lyrics investigated are the English translated version of the Korean songs and the original from the English songs of the group. I used the stop words list to exclude the common English words that has high frequency but not convey much information. I did tweak the list a little bit. First of all,\nI have to remove myself, yourself, yourselves, and ourselves from the list. As I mentioned above, BTS, supported by UNICEF, promotes the Love Myselfcampaign with the frequent existence of the motto “Love Yourself” and “Love Ourselves” in their lyrics so these words play an important role in their lyrics. Additionally, BTS usually uses some non-lexical vocables to fill space or add a rhythmic and melodic element to a song so I also added some common vocables that appear in their song.\n\nstopwordsList &lt;- as_tibble(stopwords(\"en\")) |&gt; \n  add_row(value=\"na\")  |&gt;\n  add_row(value=\"la\")  |&gt;\n  add_row(value=\"_\") |&gt;\n  add_row(value=\"oh\") |&gt;\n  add_row(value=\"yeah\") |&gt;\n  add_row(value=\"ay\") |&gt;\n  add_row(value=\"ya\") |&gt;\n  add_row(value = \"im\") |&gt;\n  add_row(value = \"i’m\") |&gt;\n  add_row(value = \"it’s\") |&gt;\n  add_row(value = \"don’t\") |&gt;\n  add_row(value = \"i’ll\") \nstopwordsList &lt;- stopwordsList[-c(4,8,12,13),]\n\nThen I split all the words in every lyric and put them all in a list. Using the stop words list, I created a frequency tibble for all the words in every BTS’ song.\n\nwordsLyrics &lt;- str_split(bts_lyrics$lyrics, \"\\\\s+\")\nwordsLyricsList &lt;- unlist(wordsLyrics) \nwordsLyricsList &lt;- wordsLyricsList[str_detect(wordsLyricsList,\"[A-Za-z']+$\")]\nwordFreq &lt;- as.data.frame(table(wordsLyricsList)) |&gt;\n  mutate(wordsLyricsList = as.character(wordsLyricsList)) |&gt;\n  anti_join(stopwordsList,by=c(wordsLyricsList=\"value\")) |&gt;\n  slice_max(Freq,n=200)\n\nUsually, when we wanted to look at the frequency of words, Word Cloud would be the most representative visualization that is applicable for this purpose.\n\nset.seed(2003)\nwordcloud2(data=wordFreq, size=1.5, color = 'random-light')\n\n\n\n\n\nFrom the word cloud, we can see that like and love are the two words with the highest frequency among all words in the lyrics. BTS always want to send the message to the ARMY that they should “love yourself” to the fullest but they mask it as a love song for the young people. That is the reason why these two words appear more frequently than others. Another noticeably frequent word is run. This word not only embodies the title track of one of their most iconic albums but also encapsulates the essence of relentlessly moving forward, a sentiment deeply resonating with the unwavering spirit of youth in the middle of adversity, a concept intrinsic to BTS. And we could see the word myself, which is a part of their successful campaign’s name that is against violence toward children and teens around the world, with the hope of making the world a better place through music.."
  },
  {
    "objectID": "string_regex.html#number-of-word-distribution-of-albums-over-the-time",
    "href": "string_regex.html#number-of-word-distribution-of-albums-over-the-time",
    "title": "Mini Project 3: Strings and Regular Expressions",
    "section": "Number of Word Distribution of Albums Over The Time",
    "text": "Number of Word Distribution of Albums Over The Time\nDuring some early years of their career, BTS delved deep into the realm of hip hop, channeling their creative energies predominantly into this genre. During these formative years, while they did experiment with other musical styles, the number of non-hip hop tracks remained relatively limited. Their debut era was characterized by a collection of rap-heavy compositions, where the lyrical content took center stage, resulting in high speechiness. However, as their artistic vision evolved and matured, BTS began to transcend genre boundaries, exploring the vast landscape of pop, R&B, and beyond. With each release, they carved out a distinct identity, showcasing their versatility and willingness to embrace diverse musical influences, thereby captivating audiences worldwide.\nI want to analyse whether this change does affect the speechiness of their songs throughout the time.\nIn their discography, BTS released three lead singles. I removed the two “Dynamite” because this song is included in the album “Be”. I also excluded “Butter/Permission To Dance” since they released these songs separately in two other singles. Then I computed the average of number of words in each album and created a graph to compare it.\n\nbts_albums &lt;- bts_lyrics |&gt; \n  mutate(lyrics_count = str_count(lyrics, \"\\\\s+\")) |&gt;\n  group_by(album_rd, eng_album_title) |&gt;\n  summarise(mean_count = mean(lyrics_count)) |&gt;\n  #remove the three lead singles\n  filter(!str_detect(eng_album_title,\"^Dynamite .\") && eng_album_title != \"Butter / Permission to Dance\")\n\n`summarise()` has grouped output by 'album_rd'. You can override using the\n`.groups` argument.\n\n\n\nbts_albums_plot &lt;- bts_albums |&gt;\n  ggplot(aes(x = album_rd, y = mean_count, text = paste(\"Album:\", eng_album_title, \"&lt;br&gt;Release Date:\", album_rd, \"&lt;br&gt;Mean Count:\", round(mean_count, 2)))) +\n  geom_point(aes(color=eng_album_title),show.legend = FALSE) +\n  labs(x = \"Release Date\", y = \"Mean Number of Words In A Song\", title = \"Average number of words in each BTS song over time\", subtitle = \"From 2013 to 2023\") +\n  theme_classic()\n\nplotly_plot &lt;- ggplotly(bts_albums_plot, tooltip = \"text\")\n\nsaveWidget(plotly_plot, file = \"plot1.html\")\n\nThis is an interactive plot:\n\nIn the albums following “Dark and Wild” (09/2014), there’s a noticeable shift in BTS’s music. The era spanning “Dark and Wild” to “The Most Beautiful Moment in Life” series marks a pivotal moment where the group explored a broader spectrum of genres. This transition brought about changes in line distribution within songs, notably favoring the Vocal team (Jin, Jimin, V, and JK) with increased singing durations. Conversely, the Rap team (RM, Suga, j-hope) saw a decline in their line distribution as rap-centric tracks became less prevalent.\nWith the diminishing emphasis on rap, the average number of words per song decreased accordingly. However, an intriguing outlier emerges with the release of “Love Yourself: Answer” in August 2018. This album stands out with the highest mean number of words, attributed partly to its larger tracklist. Additionally, each member of the Rap team presents a solo track, showcasing individual personalities and contributing to the album’s overall word count.\nTo ascertain the impact of reduced rap segments on speechiness, it’s imperative to analyze the variance in word count among songs performed by the Rap team, the Vocal team, and the group as a whole. This exploration promises insights into BTS’s evolving musical dynamics over time."
  },
  {
    "objectID": "string_regex.html#difference-in-number-of-words-performed-by-the-rap-team-the-vocal-team-and-the-whole-group",
    "href": "string_regex.html#difference-in-number-of-words-performed-by-the-rap-team-the-vocal-team-and-the-whole-group",
    "title": "Mini Project 3: Strings and Regular Expressions",
    "section": "Difference in Number of Words Performed by The Rap Team, The Vocal Team, and The Whole Group",
    "text": "Difference in Number of Words Performed by The Rap Team, The Vocal Team, and The Whole Group\n\nbts_team_songs &lt;- bts_lyrics |&gt; \n  mutate(lyrics_count = str_count(lyrics, \"\\\\s+\"),\n         performed_by = fct_collapse(performed_by,\n                                     \"Rapper\" = c(\"RM\", \"SUGA\", \"J-HOPE\", \"RM; SUGA; J-HOPE\", \"SUGA; RM\", \"SUGA; JIMIN\", \"RM; SUGA\"),\n                                     \"Vocal\" = c(\"JIN\", \"JUNGKOOK\", \"V\", \"JIMIN\", \"JUNGKOOK; V; JIMIN; JIN\", \"JIN; JIMIN; JUNGKOOK\", \"V; JIMIN\"),\n                                     \"All Team\" = c(\"BTS\"),\n                                     other_level =\"Vocal + Rap\"\n                                     ))\nggplot(bts_team_songs, aes(x = performed_by, y = lyrics_count)) +\n  geom_boxplot(outlier.size = 0.001) +\n  geom_jitter(aes(color = performed_by)) +\n  labs(title =\"Comparison the Number of Words of Song Performed by Each Team\", x = \"Performed By\", y = \"Number of Words in A Song\", color = \"Team\")\n\n\n\n\n\n\n\n\nWe can observe that despite the relatively few songs performed by the rap team, the distribution of speechiness in these songs is somewhat similar to those performed by the entire group, with three notable outliers. Conversely, the vocal team tends to perform songs with lower speechiness. This observation further strengthens my estimation regarding the influence of the genre transition on the average number of words per song produced by the group over the years.",
    "crumbs": [
      "Mini Project 3: Strings and Regular Expressions"
    ]
  },
  {
    "objectID": "string_regex.html#number-of-word-distribution-over-the-time",
    "href": "string_regex.html#number-of-word-distribution-over-the-time",
    "title": "Mini Project 3: Strings and Regular Expressions",
    "section": "Number of Word Distribution Over The Time",
    "text": "Number of Word Distribution Over The Time\nDuring some early years of their career, BTS delved deep into the realm of hip hop, channeling their creative energies predominantly into this genre. During these formative years, while they did experiment with other musical styles, the number of non-hip hop tracks remained relatively limited. Their debut era was characterized by a collection of rap-heavy compositions, where the lyrical content took center stage, resulting in high speechiness. However, as their artistic vision evolved and matured, BTS began to transcend genre boundaries, exploring the vast landscape of pop, R&B, and beyond. With each release, they carved out a distinct identity, showcasing their versatility and willingness to embrace diverse musical influences, thereby captivating audiences worldwide.\nI want to analyse whether this change does affect the speechiness of their songs throughout the time.\nIn their discography, BTS released three lead singles. I removed the two “Dynamite” because this song is included in the album “Be”. I also excluded “Butter/Permission To Dance” since they released these songs separately in two other singles. Then I computed the average of number of words in each album and created a graph to compare it.\n\nbts_albums &lt;- bts_lyrics |&gt; \n  mutate(lyrics_count = str_count(lyrics, \"\\\\s+\")) |&gt;\n  group_by(album_rd, eng_album_title) |&gt;\n  summarise(mean_count = mean(lyrics_count)) |&gt;\n  #remove the three lead singles\n  filter(!str_detect(eng_album_title,\"^Dynamite .\") && eng_album_title != \"Butter / Permission to Dance\")\n\n`summarise()` has grouped output by 'album_rd'. You can override using the\n`.groups` argument.\n\n\n\nbts_albums_plot &lt;- bts_albums |&gt;\n  ggplot(aes(x = album_rd, y = mean_count, text = paste(\"Album:\", eng_album_title, \"&lt;br&gt;Release Date:\", album_rd, \"&lt;br&gt;Mean Count:\", round(mean_count, 2)))) +\n  geom_point(aes(color = eng_album_title), size = 2, alpha = 0.8, show.legend = FALSE) +\n  labs(x = \"Release Date\", y = \"Mean Number of Words In A Song\", title = \"Average number of words in each BTS song over time\", subtitle = \"From 2013 to 2023\") +\n  theme_classic()\n\nplotly_plot &lt;- ggplotly(bts_albums_plot, tooltip = \"text\")\n\nsaveWidget(plotly_plot, file = \"plot1.html\")\n\nThis is an interactive plot:\n\nIn the albums following “Dark and Wild” (09/2014), there’s a noticeable shift in BTS’s music. The era spanning “Dark and Wild” to “The Most Beautiful Moment in Life” series marks a pivotal moment where the group explored a broader spectrum of genres. This transition brought about changes in line distribution within songs, notably favoring the Vocal team (Jin, Jimin, V, and JK) with increased singing durations. Conversely, the Rap team (RM, Suga, j-hope) saw a decline in their line distribution as rap-centric tracks became less prevalent.\nWith the diminishing emphasis on rap, the average number of words per song decreased accordingly. To ascertain the impact of reduced rap segments on speechiness, it’s imperative to analyze the variance in word count among songs performed by the Rap team, the Vocal team, and the group as a whole. This exploration promises insights into BTS’s evolving musical dynamics over time.",
    "crumbs": [
      "Mini Project 3: Strings and Regular Expressions"
    ]
  },
  {
    "objectID": "string_regex.html#what-are-the-words-used-the-most-in-bts-lyrics",
    "href": "string_regex.html#what-are-the-words-used-the-most-in-bts-lyrics",
    "title": "Mini Project 3: Strings and Regular Expressions",
    "section": "What are the words used the most in BTS’ lyrics?",
    "text": "What are the words used the most in BTS’ lyrics?\nThe lyrics investigated are the English translated version of the Korean songs and the original from the English songs of the group. I used the stop words list to exclude the common English words that has high frequency but not convey much information. I did tweak the list a little bit. First of all,\nI have to remove myself, yourself, yourselves, and ourselves from the list. As I mentioned above, BTS, supported by UNICEF, promotes the Love Myselfcampaign with the frequent existence of the motto “Love Yourself” and “Love Ourselves” in their lyrics so these words play an important role in their lyrics. Additionally, BTS usually uses some non-lexical vocables to fill space or add a rhythmic and melodic element to a song so I also added some common vocables that appear in their song.\n\nstopwordsList &lt;- as_tibble(stopwords(\"en\")) |&gt; \n  add_row(value=\"na\")  |&gt;\n  add_row(value=\"la\")  |&gt;\n  add_row(value=\"_\") |&gt;\n  add_row(value=\"oh\") |&gt;\n  add_row(value=\"yeah\") |&gt;\n  add_row(value=\"ay\") |&gt;\n  add_row(value=\"ya\") |&gt;\n  add_row(value = \"im\") |&gt;\n  add_row(value = \"i’m\") |&gt;\n  add_row(value = \"it’s\") |&gt;\n  add_row(value = \"don’t\") |&gt;\n  add_row(value = \"i’ll\") \nstopwordsList &lt;- stopwordsList[-c(4,8,12,13),]\n\nThen I split all the words in every lyric and put them all in a list. Using the stop words list, I created a frequency tibble for all the words in every BTS’ song.\n\nwordsLyrics &lt;- str_split(bts_lyrics$lyrics, \"\\\\s+\")\nwordsLyricsList &lt;- unlist(wordsLyrics) \nwordsLyricsList &lt;- wordsLyricsList[str_detect(wordsLyricsList,\"[A-Za-z']+$\")]\nwordFreq &lt;- as.data.frame(table(wordsLyricsList)) |&gt;\n  mutate(wordsLyricsList = as.character(wordsLyricsList)) |&gt;\n  anti_join(stopwordsList,by=c(wordsLyricsList=\"value\")) |&gt;\n  slice_max(Freq,n=200)\n\nUsually, when we wanted to look at the frequency of words, Word Cloud would be the most representative visualization that is applicable for this purpose.\n\nset.seed(2003)\nwordcloud2(data=wordFreq, size=1.5, color = 'random-light')\n\n\n\n\n\nFrom the word cloud, we can see that like and love are the two words with the highest frequency among all words in the lyrics. BTS always want to send the message to the ARMY that they should “love yourself” to the fullest but they mask it as a love song for the young people. That is the reason why these two words appear more frequently than others. Another noticeably frequent word is run. This word not only embodies the title track of one of their most iconic albums but also encapsulates the essence of relentlessly moving forward, a sentiment deeply resonating with the unwavering spirit of youth in the middle of adversity, a concept intrinsic to BTS. And we could see the word myself, which is a part of their successful campaign’s name that is against violence toward children and teens around the world, with the hope of making the world a better place through music..",
    "crumbs": [
      "Mini Project 3: Strings and Regular Expressions"
    ]
  },
  {
    "objectID": "sql_practice.html",
    "href": "sql_practice.html",
    "title": "SQL Practice",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mdsr)\nlibrary(dbplyr)\nlibrary(DBI)\n# connect to the scidb server on Amazon Web Services - the airlines \n# database lives on a remote server\ndb &lt;- dbConnect_scidb(\"airlines\")\nflights &lt;- tbl(db, \"flights\")\nplanes &lt;- tbl(db, \"planes\")",
    "crumbs": [
      "SQL Practice"
    ]
  },
  {
    "objectID": "sql_practice.html#on-your-own---extended-example-from-mdsr",
    "href": "sql_practice.html#on-your-own---extended-example-from-mdsr",
    "title": "SQL Practice",
    "section": "On Your Own - Extended Example from MDSR",
    "text": "On Your Own - Extended Example from MDSR\nRefer to Section 15.5 in MDSR, where they attempt to replicate FiveThirtyEight’s plot of slowest and fastest airports in the section below Figure 15.1. Instead of using target time, which has a complex definition, we will use arrival time, which oversimplifies the situation but gets us in the ballpark.\nThe MDSR authors provide a mix of SQL and R code to perform their analysis, but the code will not work if you simply cut-and-paste as-is into R. Your task is to convert the book code into something that actually runs. [Hint: use dbGetQuery()]",
    "crumbs": [
      "SQL Practice"
    ]
  },
  {
    "objectID": "sql_practice.html#on-your-own---practice-with-sql",
    "href": "sql_practice.html#on-your-own---practice-with-sql",
    "title": "SQL Practice",
    "section": "On Your Own - Practice with SQL",
    "text": "On Your Own - Practice with SQL\nThese problems are based on class exercises from MSCS 164 in Fall 2023, so you’ve already solved them in R! Now we’re going to try to duplicate those solutions in SQL.\n\n# Read in 2013 NYC flights data\nlibrary(nycflights13)\n\nWarning: package 'nycflights13' was built under R version 4.3.3\n\n\n\nAttaching package: 'nycflights13'\n\n\nThe following objects are masked _by_ '.GlobalEnv':\n\n    flights, planes\n\nflights_nyc13 &lt;- nycflights13::flights\nplanes_nyc13 &lt;- nycflights13::planes\n\n\nSummarize carriers flying to MSP by number of flights and proportion that are cancelled (assuming that a missing arrival time indicates a cancelled flight). [This was #4 in 17_longer_pipelines.Rmd.]\n\n\n# Original solution from MSCS 164\nflights_nyc13 |&gt;\n  mutate(carrier = fct_collapse(carrier, \"Delta +\" = c(\"DL\", \"9E\"), \n                                      \"American +\"= c(\"AA\", \"MQ\"), \n                                     \"United +\" = c(\"EV\", \"OO\", \"UA\"))) |&gt;\n  filter(dest == \"MSP\") |&gt;   \n  group_by(origin, carrier) |&gt;\n  summarize(n_flights = n(), \n            num_cancelled = sum(is.na(arr_time)),\n            prop_cancelled = mean(is.na(arr_time)))\n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 5 × 5\n# Groups:   origin [3]\n  origin carrier    n_flights num_cancelled prop_cancelled\n  &lt;chr&gt;  &lt;fct&gt;          &lt;int&gt;         &lt;int&gt;          &lt;dbl&gt;\n1 EWR    Delta +          598            10         0.0167\n2 EWR    United +        1779           105         0.0590\n3 JFK    Delta +         1095            41         0.0374\n4 LGA    Delta +         2420            25         0.0103\n5 LGA    American +      1293            62         0.0480\n\n\nFirst duplicate the output above, then check trends across all years and origins. Here are a few hints:\n\nuse flights instead of flights_nyc13\nremember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)\nis.na can be replaced with CASE WHEN arr_time = ‘NA’ THEN 1 ELSE 0 END\nCASE WHEN can also be used replace fct_collapse\n\n\nSELECT carrier, dest, arr_time, origin, year,\n  SUM(1) AS n_flights, \n  SUM(CASE WHEN arr_time  = 'NA' THEN 1 ELSE 0 END) as num_cancelled, \n  AVG(CASE WHEN arr_time  = 'NA' THEN 1 ELSE 0 END) as prop_cancelled, \n  CASE WHEN (carrier = 'DL' OR carrier = 'E9') THEN 'Delta +'\n       WHEN (carrier = 'AA' OR carrier = 'MQ') THEN 'American +'\n       WHEN (carrier = 'EV' OR carrier = 'OO' OR carrier = \"UA\") THEN 'United +'\n       ELSE 'Other' END AS new_carrier\nFROM flights\nWHERE dest = \"MSP\" AND year = 2013 AND (origin = \"EWR\" OR origin = \"JFK\" OR origin = \"LGA\")\nGROUP BY origin, new_carrier\nORDER BY prop_cancelled DESC;\n\n\n8 records\n\n\n\n\n\n\n\n\n\n\n\n\n\ncarrier\ndest\narr_time\norigin\nyear\nn_flights\nnum_cancelled\nprop_cancelled\nnew_carrier\n\n\n\n\nEV\nMSP\n800\nEWR\n2013\n1779\n105\n0.0590\nUnited +\n\n\nMQ\nMSP\n847\nLGA\n2013\n1293\n62\n0.0480\nAmerican +\n\n\n9E\nMSP\n954\nJFK\n2013\n1070\n41\n0.0383\nOther\n\n\n9E\nMSP\n1310\nEWR\n2013\n178\n5\n0.0281\nOther\n\n\nDL\nMSP\n1007\nEWR\n2013\n420\n5\n0.0119\nDelta +\n\n\nDL\nMSP\n751\nLGA\n2013\n2419\n25\n0.0103\nDelta +\n\n\nDL\nMSP\n1018\nJFK\n2013\n25\n0\n0.0000\nDelta +\n\n\n9E\nMSP\n2225\nLGA\n2013\n1\n0\n0.0000\nOther\n\n\n\n\n\n\nPlot number of flights vs. proportion cancelled for every origin-destination pair (assuming that a missing arrival time indicates a cancelled flight). [This was #7 in 17_longer_pipelines.Rmd.]\n\n\n# Original solution from MSCS 164\nres2 &lt;- flights_nyc13 |&gt;\n  group_by(origin, dest) |&gt;\n  summarize(n = n(),\n            prop_cancelled = mean(is.na(arr_time))) |&gt;\n  filter(prop_cancelled &lt; 1)\n\n`summarise()` has grouped output by 'origin'. You can override using the\n`.groups` argument.\n\nres2\n\n# A tibble: 223 × 4\n# Groups:   origin [3]\n   origin dest      n prop_cancelled\n   &lt;chr&gt;  &lt;chr&gt; &lt;int&gt;          &lt;dbl&gt;\n 1 EWR    ALB     439        0.0478 \n 2 EWR    ANC       8        0      \n 3 EWR    ATL    5022        0.0271 \n 4 EWR    AUS     968        0.00930\n 5 EWR    AVL     265        0.0453 \n 6 EWR    BDL     443        0.0700 \n 7 EWR    BNA    2336        0.0385 \n 8 EWR    BOS    5327        0.0146 \n 9 EWR    BQN     297        0.00673\n10 EWR    BTV     931        0.0473 \n# ℹ 213 more rows\n\nres2 |&gt;  ggplot(aes(n, prop_cancelled)) + \n  geom_point()\n\n\n\n\n\n\n\n\n\nSELECT origin, dest, arr_time,\n    SUM(1) AS n, \n    AVG(CASE WHEN arr_time  = 'NA' THEN 1 ELSE 0 END) AS prop_cancelled\nFROM flights\nWHERE year = 2013 AND (origin = \"EWR\" OR origin = \"JFK\" OR origin = \"LGA\") \nGROUP BY origin, dest\nHAVING prop_cancelled &lt; 1\n\n\nres2sql |&gt;  ggplot(aes(n, prop_cancelled)) + \n  geom_point()\n\n\n\n\n\n\n\n\nFirst duplicate the plot above, then check trends across all years and origins. Do all of the data wrangling in SQL. Here are a few hints:\n\nuse flights instead of flights_nyc13\nremember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)\nuse an sql chunk and an r chunk\ninclude connection = and output.var = in your sql chunk header (this doesn’t seem to work with dbGetQuery()…)\n\n\nProduce a table of weighted plane age by carrier, where weights are based on number of flights per plane. [This was #6 in 26_more_joins.Rmd.]\n\n\n# Original solution from MSCS 164\nflights_nyc13 |&gt;\n  left_join(planes_nyc13, join_by(tailnum)) |&gt;\n  mutate(plane_age = 2013 - year.y) |&gt;\n  group_by(carrier) |&gt;\n  summarize(unique_planes = n_distinct(tailnum),\n            mean_weighted_age = mean(plane_age, na.rm =TRUE),\n            sd_weighted_age = sd(plane_age, na.rm =TRUE)) |&gt;\n  arrange(mean_weighted_age)\n\n# A tibble: 16 × 4\n   carrier unique_planes mean_weighted_age sd_weighted_age\n   &lt;chr&gt;           &lt;int&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n 1 HA                 14              1.55            1.14\n 2 AS                 84              3.34            3.07\n 3 VX                 53              4.47            2.14\n 4 F9                 26              4.88            3.67\n 5 B6                193              6.69            3.29\n 6 OO                 28              6.84            2.41\n 7 9E                204              7.10            2.67\n 8 US                290              9.10            4.88\n 9 WN                583              9.15            4.63\n10 YV                 58              9.31            1.93\n11 EV                316             11.3             2.29\n12 FL                129             11.4             2.16\n13 UA                621             13.2             5.83\n14 DL                629             16.4             5.49\n15 AA                601             25.9             5.42\n16 MQ                238             35.3             3.13\n\n\nFirst duplicate the output above, then check trends across all years and origins. Do all of the data wrangling in SQL. Here are a few hints:\n\nSELECT f.carrier, COUNT(DISTINCT f.tailnum) AS unique_planes, avg((2013 - p.year)) AS mean_weighted_age, STDDEV(2013 - p.year) AS sd_weighted_age \nFROM flights AS f\nLEFT JOIN planes AS p ON f.tailnum = p.tailnum\nWHERE f.year = 2013 AND f.year != 'NA' AND (origin = \"EWR\" OR origin = \"JFK\" OR origin = \"LGA\") \nGROUP BY f.carrier\nORDER BY mean_weighted_age;\n\n\nDisplaying records 1 - 10\n\n\ncarrier\nunique_planes\nmean_weighted_age\nsd_weighted_age\n\n\n\n\nHA\n14\n1.5484\n1.137022\n\n\nAS\n84\n3.3366\n3.068823\n\n\nVX\n53\n4.4736\n2.135061\n\n\nF9\n26\n4.8787\n3.665043\n\n\nB6\n193\n6.6867\n3.289461\n\n\nOO\n28\n6.8438\n2.373149\n\n\n9E\n204\n7.1011\n2.669565\n\n\nUS\n290\n9.1037\n4.881782\n\n\nWN\n583\n9.1461\n4.625865\n\n\nYV\n58\n9.3138\n1.925773\n\n\n\n\n\n\nuse flights instead of flights_nyc13\nremember that flights_nyc13 only contained 2013 and 3 NYC origin airports (EWR, JFK, LGA)\nyou’ll have to merge the flights dataset with the planes dataset\nyou can use DISTINCT inside a COUNT()\n\n\ndbDisconnect(db)",
    "crumbs": [
      "SQL Practice"
    ]
  }
]